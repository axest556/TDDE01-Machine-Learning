install.packages("neuralnet")
library(neuralnet)
#Set SEED
set.seed(1234567890)
#### Assignment 4.1 ####
#Given code from assignment
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(10,-1,1)
nn_logi <- neuralnet(Sin ~ Var, tr, hidden = 10, startweights = winit)
# Plot of the training data (black), test data (blue), and predictions on the test data (red)
plot(tr, cex=2, main="Training Data, Test Data, and NN Predictions")
points(te, col = "blue", cex=1)
points(te[,1],predict(nn_logi,te), col="red", cex=1)
legend("bottomleft",
legend = c("Training Data", "Test Data", "NN Predictions"),
col = c("black", "blue", "red"),
pch = 1,
cex = 0.8)
#### Assignment 4.2 ####
# Neural network with the activation function to be linear f(x) = x
nn_linear <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = function(x) x, startweights = winit)
#ReLU function
ReLU <- function (x){
ifelse(x >= 0, x, 0) #in the report explain why we did  not use return or pmax()
}
# Neural network with the activation function ReLU f(x)=x,  ifelse(x >= 0, x, 0)
nn_ReLU <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = ReLU, startweights = winit)
#Softplus function
softplus <- function(x){
log(1+exp(x))
}
# Neural network with the activation function softplus f(x)= log(1+exp(x)
nn_softplus <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = softplus, startweights = winit)
plot(tr, cex=2, ylim = c(-1, 1.5), main="Predictions of NN models with Linear, ReLU, and Softplus\n activation functions, compared to Training and Test data"
)
points(te, col = "blue") # Test data points
points(te[,1], predict(nn_linear, te), col="red") # Linear model predictions
points(te[,1], predict(nn_ReLU, te), col="green") # ReLU model predictions
points(te[,1], predict(nn_softplus, te), col="orange") # Softplus model predictions
legend("bottomleft",
legend = c("Training Data", "Test Data", "Linear", "ReLU", "Softplus"),
col = c("black", "blue", "red", "green", "orange"),
pch = 1, cex = 0.8)
#### Assignment 4.3 ####
Var<- runif(500, 0, 50)
mydata2 <- data.frame(Var, Sin=sin(Var))
test2 <- mydata2[1:500,]
# Set up the plot with x-axis limits between 0 and 50
plot(tr, cex = 1, xlim = c(0, 50), ylim = c(-12, 1.5), main = "Sine Function Predictions", xlab = "Var", ylab = "Sin(Var)")
# Add test data points in blue
points(test2, col = "blue", cex = 1)
# Add predictions from the neural network in red
points(test2[, 1], predict(nn_logi, test2), col = "red", cex=1)
abline(h = -9, col = "green", lwd = 2)
legend("bottomleft", legend = c("Training Data", "Test Data", "NN Predictions
on test data", "sin(var) = - 9"),
col = c("black", "blue", "red", "green"), pch = c(1, 1, 1), cex = 0.8)
#### Assignment 4.4 ####
# Print the weights
print(nn_logi$weights)
# Look at the weights
#### Assignment 4.5 ####
# Sample points
set.seed(1234567890)  # Reset seed for reproducibility
Var <- runif(500, 0, 10)
# Create data frame but now sin(x) is input and x is output!
mydata_inverse <- data.frame(Var = Var, Sin = sin(Var))
# Train neural network with flipped relationship
# Sin ~ Var (from earlier) becomes Var ~ Sin
nn_inverse <- neuralnet(Var ~ Sin, mydata, hidden = 10, threshold = 0.1)
# Plot results
plot(mydata_inverse$Sin, mydata_inverse$Var,
main = "Predicting x from sin(x)",
xlab = "sin(x)",
ylab = "x",
col = "blue",
cex = 0.7)
points(mydata_inverse$Sin, predict(nn_inverse, mydata_inverse),
col = "red",
cex = 0.7)
legend("bottomleft",
legend = c("Actual", "Predicted"),
col = c("blue", "red"),
pch = 1,
cex = 0.8)
install.packages("neuralnet")
library(neuralnet)
#Set SEED
set.seed(1234567890)
#### Assignment 4.1 ####
#Given code from assignment
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(10,-1,1)
nn_logi <- neuralnet(Sin ~ Var, tr, hidden = 10, startweights = winit)
# Plot of the training data (black), test data (blue), and predictions on the test data (red)
plot(tr, cex=2, main="Training Data, Test Data, and NN Predictions")
points(te, col = "blue", cex=1)
points(te[,1],predict(nn_logi,te), col="red", cex=1)
legend("bottomleft",
legend = c("Training Data", "Test Data", "NN Predictions"),
col = c("black", "blue", "red"),
pch = 1,
cex = 0.8)
#### Assignment 4.2 ####
# Neural network with the activation function to be linear f(x) = x
nn_linear <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = function(x) x, startweights = winit)
#ReLU function
ReLU <- function (x){
ifelse(x >= 0, x, 0) #in the report explain why we did  not use return or pmax()
}
# Neural network with the activation function ReLU f(x)=x,  ifelse(x >= 0, x, 0)
nn_ReLU <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = ReLU, startweights = winit)
#Softplus function
softplus <- function(x){
log(1+exp(x))
}
# Neural network with the activation function softplus f(x)= log(1+exp(x)
nn_softplus <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = softplus, startweights = winit)
plot(tr, cex=2, ylim = c(-1, 1.5), main="Predictions of NN models with Linear, ReLU, and Softplus\n activation functions, compared to Training and Test data"
)
points(te, col = "blue") # Test data points
points(te[,1], predict(nn_linear, te), col="red") # Linear model predictions
points(te[,1], predict(nn_ReLU, te), col="green") # ReLU model predictions
points(te[,1], predict(nn_softplus, te), col="orange") # Softplus model predictions
legend("bottomleft",
legend = c("Training Data", "Test Data", "Linear", "ReLU", "Softplus"),
col = c("black", "blue", "red", "green", "orange"),
pch = 1, cex = 0.8)
#### Assignment 4.3 ####
Var<- runif(500, 0, 50)
mydata2 <- data.frame(Var, Sin=sin(Var))
test2 <- mydata2[1:500,]
# Set up the plot with x-axis limits between 0 and 50
plot(tr, cex = 1, xlim = c(0, 50), ylim = c(-12, 1.5), main = "Sine Function Predictions", xlab = "Var", ylab = "Sin(Var)")
# Add test data points in blue
points(test2, col = "blue", cex = 1)
# Add predictions from the neural network in red
points(test2[, 1], predict(nn_logi, test2), col = "red", cex=1)
abline(h = -9, col = "green", lwd = 2)
legend("bottomleft", legend = c("Training Data", "Test Data", "NN Predictions
on test data", "sin(var) = - 9"),
col = c("black", "blue", "red", "green"), pch = c(1, 1, 1), cex = 0.8)
#### Assignment 4.4 ####
# Print the weights
print(nn_logi$weights)
# Look at the weights
#### Assignment 4.5 ####
# Sample points
set.seed(1234567890)  # Reset seed for reproducibility
Var <- runif(500, 0, 10)
# Create data frame but now sin(x) is input and x is output!
mydata_inverse <- data.frame(Var = Var, Sin = sin(Var))
# Train neural network with flipped relationship
# Sin ~ Var (from earlier) becomes Var ~ Sin
nn_inverse <- neuralnet(Var ~ Sin, mydata, hidden = 10, threshold = 0.1)
# Plot results
plot(mydata_inverse$Sin, mydata_inverse$Var,
main = "Predicting x from sin(x)",
xlab = "sin(x)",
ylab = "x",
col = "blue",
cex = 0.7)
points(mydata_inverse$Sin, predict(nn_inverse, mydata_inverse),
col = "red",
cex = 0.7)
legend("bottomleft",
legend = c("Actual", "Predicted"),
col = c("blue", "red"),
pch = 1,
cex = 0.8)
install.packages("neuralnet")
library(neuralnet)
#Set SEED
set.seed(1234567890)
#### Assignment 4.1 ####
#Given code from assignment
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(10,-1,1)
nn_logi <- neuralnet(Sin ~ Var, tr, hidden = 10, startweights = winit)
# Plot of the training data (black), test data (blue), and predictions on the test data (red)
plot(tr, cex=2, main="Training Data, Test Data, and NN Predictions")
points(te, col = "blue", cex=1)
points(te[,1],predict(nn_logi,te), col="red", cex=1)
legend("bottomleft",
legend = c("Training Data", "Test Data", "NN Predictions"),
col = c("black", "blue", "red"),
pch = 1,
cex = 0.8)
#### Assignment 4.2 ####
# Neural network with the activation function to be linear f(x) = x
nn_linear <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = function(x) x, startweights = winit)
#ReLU function
ReLU <- function (x){
ifelse(x >= 0, x, 0) #in the report explain why we did  not use return or pmax()
}
# Neural network with the activation function ReLU f(x)=x,  ifelse(x >= 0, x, 0)
nn_ReLU <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = ReLU, startweights = winit)
#Softplus function
softplus <- function(x){
log(1+exp(x))
}
# Neural network with the activation function softplus f(x)= log(1+exp(x)
nn_softplus <- neuralnet(Sin ~ Var, tr, hidden = 10, act.fct = softplus, startweights = winit)
plot(tr, cex=2, ylim = c(-1, 1.5), main="Predictions of NN models with Linear, ReLU, and Softplus\n activation functions, compared to Training and Test data"
)
points(te, col = "blue") # Test data points
points(te[,1], predict(nn_linear, te), col="red") # Linear model predictions
points(te[,1], predict(nn_ReLU, te), col="green") # ReLU model predictions
points(te[,1], predict(nn_softplus, te), col="orange") # Softplus model predictions
legend("bottomleft",
legend = c("Training Data", "Test Data", "Linear", "ReLU", "Softplus"),
col = c("black", "blue", "red", "green", "orange"),
pch = 1, cex = 0.8)
#### Assignment 4.3 ####
set.seed(1234567890)
Var<- runif(500, 0, 50)
mydata2 <- data.frame(Var, Sin=sin(Var))
test2 <- mydata2[1:500,]
# Set up the plot with x-axis limits between 0 and 50
plot(tr, cex = 1, xlim = c(0, 50), ylim = c(-12, 1.5), main = "Sine Function Predictions", xlab = "Var", ylab = "Sin(Var)")
# Add test data points in blue
points(test2, col = "blue", cex = 1)
# Add predictions from the neural network in red
points(test2[, 1], predict(nn_logi, test2), col = "red", cex=1)
abline(h = -9, col = "green", lwd = 2)
legend("bottomleft", legend = c("Training Data", "Test Data", "NN Predictions
on test data", "sin(var) = - 9"),
col = c("black", "blue", "red", "green"), pch = c(1, 1, 1), cex = 0.8)
#### Assignment 4.4 ####
# Print the weights
print(nn_logi$weights)
# Look at the weights
#### Assignment 4.5 ####
# Sample points
set.seed(1234567890)  # Reset seed for reproducibility
Var <- runif(500, 0, 10)
# Create data frame but now sin(x) is input and x is output!
mydata_inverse <- data.frame(Var = Var, Sin = sin(Var))
# Train neural network with flipped relationship
# Sin ~ Var (from earlier) becomes Var ~ Sin
nn_inverse <- neuralnet(Var ~ Sin, mydata, hidden = 10, threshold = 0.1)
# Plot results
plot(mydata_inverse$Sin, mydata_inverse$Var,
main = "Predicting x from sin(x)",
xlab = "sin(x)",
ylab = "x",
col = "blue",
cex = 0.7)
points(mydata_inverse$Sin, predict(nn_inverse, mydata_inverse),
col = "red",
cex = 0.7)
legend("bottomleft",
legend = c("Actual", "Predicted"),
col = c("blue", "red"),
pch = 1,
cex = 0.8)
#### LAB 3, ASSIGNMENT 2 ####
#### SET SEED ####
set.seed(1234567890)
#### Install necessary packages ####
install.packages("geosphere")
library(geosphere)
#### Code given in the lab ####
stations <- read.csv("stations.csv", fileEncoding = "latin1")
setwd("~/Desktop/TDDE01/tdde01-machinelearning/Labs/lab3/Data")
#### LAB 3, ASSIGNMENT 2 ####
#### SET SEED ####
set.seed(1234567890)
#### Install necessary packages ####
install.packages("geosphere")
library(geosphere)
#### Code given in the lab ####
stations <- read.csv("stations.csv", fileEncoding = "latin1")
temps <- read.csv("temps50k.csv")
st <- merge(stations,temps,by="station_number")
bandwidth_geo <- 50000 # distance in meters (..?)
bandwidth_date <- 5 # number of days
bandwidth_time <- 4 # number of hours
# The point to predict
latitude <- 65.63141
longitude <- 22.02253
interest_date <- as.Date("2015-01-15")
prediction_hours <- c(4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24)
# the vectors that will be filled with predictions
pred_temperatures_sum <- vector(length=length(prediction_hours))
pred_temperatures_prod <- vector(length=length(prediction_hours))
# Ensure the `date` column is in Date format
st$date <- as.Date(st$date)
# Filter the data to keep rows with dates on or before the interest date
filtered_data <- subset(st, date <= interest_date)
# Function used to calculate the gaussian
gaussian_kernel <- function(x, h){
return(exp(-(x^2) / (2 * h^2)))
}
### Gaussian kernel for distance
geo_distances <- distHaversine(
cbind(filtered_data$longitude, filtered_data$latitude),
c(longitude, latitude)
)
K_geo <- gaussian_kernel(geo_distances, bandwidth_geo)
### Gaussian kernel for date
date_distances <- (as.numeric(difftime(interest_date, filtered_data$date, units = "days")))
K_date <- gaussian_kernel(date_distances, bandwidth_date)
# Extract hours from the time strings in filtered_data
filtered_data$hour <- as.numeric(substr(filtered_data$time, 1, 2))
# Calculate predictions for each hour we want to predict
for(i in 1 : length(prediction_hours)){
target_hour <- prediction_hours[i] # The hour we're trying to predict
# Calculate time differences
time_distances <- abs(filtered_data$hour - target_hour)
# for example, 23 and 01 should have a 2 hour difference, not 23-1=22
time_distances <- pmin(time_distances, 24 - time_distances)
# Calculate time kernel
K_time <- gaussian_kernel(time_distances, bandwidth_time)
# Combine kernels by summing
K_sum_combined <- K_geo + K_date + K_time
# Combine kernels by multiplying
K_prod_combined <- K_geo * K_date * K_time
# Calculate weighted average temperatures (both methods)
pred_temperatures_sum[i] <- sum(K_sum_combined * filtered_data$air_temperature) / sum(K_sum_combined)
pred_temperatures_prod[i] <- sum(K_prod_combined * filtered_data$air_temperature) / sum(K_prod_combined)
# Product prediction lower, more "strict" (needs to be close in all directions)
cat("Time:", target_hour, "- Sum:", round(pred_temperatures_sum[i], 4),
"°C, Product:", round(pred_temperatures_prod[i], 4), "°C\n")
}
### PLOT THE KERNEL VALUE AS A FUNCTION OF DISTANCE
# Create sequence of distances/differences to plot
geo_seq <- seq(0, 120000, length.out=100)  # 0 to 100km
date_seq <- seq(0, 14, length.out=100)     # 0 to 14 days
time_seq <- seq(0, 12, length.out=100)     # 0 to 12 hours
# Calculate kernel values
geo_kernel_values <- gaussian_kernel(geo_seq, bandwidth_geo)
date_kernel_values <- gaussian_kernel(date_seq, bandwidth_date)
time_kernel_values <- gaussian_kernel(time_seq, bandwidth_time)
# Create plots
par(mfrow=c(1,3))  # 1 row, 3 columns of plots
# Geographic distance kernel
plot(geo_seq/1000, geo_kernel_values, type="l",
xlab="Geographic Distance (km)", ylab="Kernel Value",
main="Geographic Kernel")
# Date distance kernel
plot(date_seq, date_kernel_values, type="l",
xlab="Date Distance (days)", ylab="Kernel Value",
main="Date Kernel")
# Time distance kernel
plot(time_seq, time_kernel_values, type="l",
xlab="Time Distance (hours)", ylab="Kernel Value",
main="Time Kernel")
# EXTRA, not part of lab but to see the and compare to actual values
# close in geographical distance and date:
# Sort all measurements by distance to our point
st$distance_to_lulea <- distHaversine(
cbind(st$longitude, st$latitude),
c(22.02253, 65.63141)
)
# Find stations close to Luleå (within 100km)
nearby_stations <- subset(st, distHaversine(cbind(longitude, latitude), c(22.02253, 65.63141)) < 100000)
# Look at measurements near our date of interest (within 2 weeks)
nearby_data <- subset(nearby_stations,
abs(as.numeric(difftime(date, as.Date("2015-01-15"), units="days"))) < 14)
# Print relevant information
print("Nearby measurements:")
print(nearby_data[, c("station_number", "station_name", "date", "time", "air_temperature")])
# Looking at this data, the product predictions seem better,
# the sum predictions guessed too high
View(st)
